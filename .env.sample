# Chave da API do Google para LLM (Gemini)
GOOGLE_API_KEY=SUA_CHAVE_DE_API_DO_GEMINI_AQUI

# Chave da API da OpenAI (opcional, apenas se usar embeddings da OpenAI)
# OPENAI_API_KEY=SUA_CHAVE_DA_OPENAI_AQUI

# Configuração do modelo de embeddings
# Opções disponíveis:
# - "google": Usa o modelo de embeddings do Google (requer GOOGLE_API_KEY)
# - "openai": Usa o modelo de embeddings da OpenAI (requer OPENAI_API_KEY)
# - "huggingface": Usa modelos do Hugging Face (local, não precisa de API)
# - "local": Usa modelos locais via sentence-transformers (padrão)
EMBEDDING_PROVIDER=local

# Nome do modelo de embeddings (opcional, cada provedor tem um padrão)
# Exemplos:
# - Para Google: "models/embedding-001"
# - Para OpenAI: "text-embedding-3-small"
# - Para HuggingFace/Local: "intfloat/multilingual-e5-small" ou "paraphrase-multilingual-MiniLM-L12-v2"
# EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2

# Parâmetros avançados para o modelo de embeddings (opcional, formato JSON)
# Exemplos:
# - Para Google: '{"task_type": "retrieval_document", "title": "Documentos UFPI"}'
# - Para OpenAI: '{"chunk_size": 1000, "show_progress_bar": true}'
# - Para HuggingFace/Local: '{"encode_kwargs": {"batch_size": 32, "normalize_embeddings": true}, "model_kwargs": {"device": "cpu"}}'
# EMBEDDING_KWARGS={"encode_kwargs": {"normalize_embeddings": true}, "model_kwargs": {"device": "cpu"}}

# Configuração do retriever (opcional, formato JSON)
# Exemplo para busca padrão:
# RETRIEVER_CONFIG={"search_type": "similarity", "k": 5}
#
# Exemplo para Maximum Marginal Relevance (equilibra relevância e diversidade):
# RETRIEVER_CONFIG={"search_type": "mmr", "k": 5, "fetch_k": 20, "lambda_mult": 0.7}
#
# Exemplo para filtrar por pontuação mínima:
# RETRIEVER_CONFIG={"search_type": "similarity_score_threshold", "k": 10, "score_threshold": 0.75}